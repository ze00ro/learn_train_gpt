## nlp

[Huggingface NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)

[Transformer](https://huggingface.co/docs/transformers/index)

## train

[根据已有的tokenizer训练新的tokenizer](https://huggingface.co/learn/nlp-course/zh-CN/chapter6/2?fw=pt)

[How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train)

[Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)

[How to train a language model from scratch](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)

[How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)

[Fine-tune a non-English GPT-2 Model with Huggingface](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)

## peft (参数高效微调)

[使用 LoRA 和 Hugging Face 高效训练大语言模型](https://www.cnblogs.com/huggingface/p/17311912.html)

## generate

[How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)
[(中文)](https://www.cnblogs.com/huggingface/p/17351307.html)

[基于 transformers 的 generate 方法实现多样化文本生成](https://blog.csdn.net/muyao987/article/details/125917234)

[基于 gpt2 实现文本生成](https://zhuanlan.zhihu.com/p/495421804)

## other

[训练自己的GPT2模型（中文），踩坑与经验](https://blog.csdn.net/qq_51750957/article/details/128856264)